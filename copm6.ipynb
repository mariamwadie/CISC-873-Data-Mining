{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "6ua7ZMaOsB_o",
        "outputId": "0aae8d8c-07d3-47e6-e0f5-ab6e36e924a6"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/25024 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#importing libraries\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "import tensorflow\n",
        "\n",
        "def read_sdf(file):\n",
        "    with open(file, 'r') as rf:\n",
        "        content = rf.read()\n",
        "    samples = content.split('$$$$')\n",
        "\n",
        "\n",
        "    def parse_sample(s):\n",
        "        lines = s.splitlines()    #splitting the text data to lines\n",
        "        links = []                #empty array for links\n",
        "        nodes = []                #empty array for nodes\n",
        "        label = 0\n",
        "        for l in lines:           #loop over each line\n",
        "            if l.strip() == '1.0':   #\n",
        "                label = 1\n",
        "            if l.strip() == '-1.0':\n",
        "                label = 0\n",
        "            if l.startswith('    '):\n",
        "                feature = l.split()\n",
        "                node = feature[3]\n",
        "                nodes.append(node)\n",
        "            elif l.startswith(' '):\n",
        "                lnk = l.split()\n",
        "                if int(lnk[0]) - 1 < len(nodes):\n",
        "                    links.append((\n",
        "                        int(lnk[0])-1,\n",
        "                        int(lnk[1])-1,\n",
        "\n",
        "                    ))\n",
        "        return nodes, np.array(links), label\n",
        "\n",
        "    return [parse_sample(s) for s in tqdm(samples) if len(s[0]) > 0]\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "#reading train.sdf file\n",
        "training_set = read_sdf('train.sdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "NBkoMfUVsB_0",
        "outputId": "633d96d9-b240-4935-b895-add63907b0d5"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/12326 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "#reading test file\n",
        "testing_set  = read_sdf('test_x.sdf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi_l_NaKsB_7"
      },
      "outputs": [],
      "source": [
        "#Tokenizer importing\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#vocabulary size\n",
        "max_vocab = 500\n",
        "max_len = 100 # maximum length of the tokenized vector\n",
        "\n",
        "\n",
        "\n",
        "all_nodes = [s[0] for s in training_set]\n",
        "\n",
        "#training tokenizer\n",
        "tokenizer = Tokenizer(num_words=max_vocab)\n",
        "tokenizer.fit_on_texts(all_nodes)\n",
        "#importing libraries\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import random\n",
        "random.seed(0) #random seed\n",
        "\n",
        "\n",
        "\n",
        "def prepare_single_batch(samples):\n",
        "    sample_nodes = [s[0] for s in samples]\n",
        "    sample_nodes = tokenizer.texts_to_sequences(sample_nodes)\n",
        "    sample_nodes = pad_sequences(sample_nodes, padding='post')\n",
        "    max_nodes_len = np.shape(sample_nodes)[1]\n",
        "    edges = [s[1]+i*max_nodes_len for i,s in enumerate(samples)]\n",
        "    edges = [e for e in edges if len(e) > 0]\n",
        "\n",
        "\n",
        "    node_to_graph = [[i]*max_nodes_len for i in range(len(samples))]\n",
        "\n",
        "    all_nodes = np.reshape(sample_nodes, -1)\n",
        "    all_edges = np.concatenate(edges)\n",
        "\n",
        "    node_to_graph = np.reshape(node_to_graph, -1)\n",
        "    return {\n",
        "        'data': all_nodes,\n",
        "        'edges': all_edges,\n",
        "        'node2grah': node_to_graph,\n",
        "    }, np.array([s[2] for s in samples])\n",
        "\n",
        "\n",
        "def gen_batch(dataset, batch_size=16, repeat=False, shuffle=True):\n",
        "    while True:\n",
        "        dataset = list(dataset)\n",
        "        if shuffle:\n",
        "            random.shuffle(dataset)\n",
        "        l = len(dataset)\n",
        "        for ndx in range(0, l, batch_size):\n",
        "            batch_samples = dataset[ndx:min(ndx + batch_size, l)]\n",
        "            yield prepare_single_batch(batch_samples)\n",
        "        if not repeat:\n",
        "            break\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kvHhC0_sB_9"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet tf2_gnn\n",
        "from tf2_gnn.layers.gnn import GNN, GNNInput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "37SGEG25sCAA",
        "outputId": "95e842ba-6666-47d4-c630-4626b5d55a28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "gnn_out KerasTensor(type_spec=TensorSpec(shape=(None, 40), dtype=tf.float32, name=None), name='gnn/StatefulPartitionedCall:0', description=\"created by layer 'gnn'\")\n",
            "mean: KerasTensor(type_spec=TensorSpec(shape=(None, 40), dtype=tf.float32, name=None), name='tf.math.segment_mean/SegmentMean:0', description=\"created by layer 'tf.math.segment_mean'\")\n",
            "pred: KerasTensor(type_spec=TensorSpec(shape=(None, 1), dtype=tf.float32, name=None), name='dense/Sigmoid:0', description=\"created by layer 'dense'\")\n",
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_3 (InputLayer)           [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " input_1 (InputLayer)           [(None,)]            0           []                               \n",
            "                                                                                                  \n",
            " tf.math.reduce_max (TFOpLambda  ()                  0           ['input_3[0][0]']                \n",
            " )                                                                                                \n",
            "                                                                                                  \n",
            " embedding (Embedding)          (None, 100)          50000       ['input_1[0][0]']                \n",
            "                                                                                                  \n",
            " input_2 (InputLayer)           [(None, 2)]          0           []                               \n",
            "                                                                                                  \n",
            " tf.__operators__.add (TFOpLamb  ()                  0           ['tf.math.reduce_max[0][0]']     \n",
            " da)                                                                                              \n",
            "                                                                                                  \n",
            " gnn (GNN)                      (None, 40)           35440       ['embedding[0][0]',              \n",
            "                                                                  'input_2[0][0]',                \n",
            "                                                                  'input_3[0][0]',                \n",
            "                                                                  'tf.__operators__.add[0][0]']   \n",
            "                                                                                                  \n",
            " tf.math.segment_mean (TFOpLamb  (None, 40)          0           ['gnn[0][0]',                    \n",
            " da)                                                              'input_3[0][0]']                \n",
            "                                                                                                  \n",
            " dense (Dense)                  (None, 1)            41          ['tf.math.segment_mean[0][0]']   \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 85,481\n",
            "Trainable params: 85,481\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.math import segment_mean #to calculate segmented mean\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, Model #layers and model\n",
        "from tensorflow.keras.layers import Embedding, Dense #layers\n",
        "from tensorflow.keras.optimizers import Adam #optimizer\n",
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 100)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"hidden_dim\"] = 40\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "\n",
        "pred = Dense(1, activation='sigmoid')(avg)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4gL9oFD7sCAC"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SVU3zF-csCAE"
      },
      "outputs": [],
      "source": [
        "zero_class=[]\n",
        "one_class=[]\n",
        "for i in range(len(training_set)):\n",
        "\n",
        "    if training_set[i][2]==0:\n",
        "\n",
        "        zero_class.append(training_set[i])\n",
        "    else:\n",
        "        one_class.append(training_set[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w8hK2MJMsCAF"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "one_upsample = resample(one_class,\n",
        "             replace=True,\n",
        "             n_samples=len(zero_class),\n",
        "             random_state=19)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hvSPXFwJsCAG"
      },
      "outputs": [],
      "source": [
        "training_balanced = [*zero_class,*one_upsample]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "apF1cNJfsCAG"
      },
      "outputs": [],
      "source": [
        "training_balanced, validation_balanced = train_test_split(training_balanced, test_size=0.15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tXAT2EzWsCAK",
        "outputId": "4eabd52b-c457-4526-fc34-f88c6e9db8a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\Mariam W. B. Hana\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_3_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_3_grad/Identity:0\", shape=(None, 40), dtype=float32), dense_shape=Tensor(\"gradients/cond_3_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Mariam W. B. Hana\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_1_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_1_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_1_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Mariam W. B. Hana\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/GatherV2_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/GatherV2_grad/Reshape:0\", shape=(None,), dtype=float32), dense_shape=Tensor(\"gradients/GatherV2_grad/Cast:0\", shape=(1,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Mariam W. B. Hana\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Reshape:0\", shape=(None, 40), dtype=float32), dense_shape=Tensor(\"gradients/rgcn_2/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Mariam W. B. Hana\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/cond_1_grad/Identity_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/cond_1_grad/Identity:0\", shape=(None, 40), dtype=float32), dense_shape=Tensor(\"gradients/cond_1_grad/Identity_2:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n",
            "C:\\Users\\Mariam W. B. Hana\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\indexed_slices.py:444: UserWarning: Converting sparse IndexedSlices(IndexedSlices(indices=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape_1:0\", shape=(None,), dtype=int32), values=Tensor(\"gradients/rgcn/embedding_lookup_grad/Reshape:0\", shape=(None, 40), dtype=float32), dense_shape=Tensor(\"gradients/rgcn/embedding_lookup_grad/Cast:0\", shape=(2,), dtype=int32))) to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "5059/5059 [==============================] - 223s 43ms/step - loss: 0.6186 - auc: 0.7199 - val_loss: 0.5860 - val_auc: 0.7635\n",
            "Epoch 2/50\n",
            "5059/5059 [==============================] - 253s 50ms/step - loss: 0.5733 - auc: 0.7727 - val_loss: 0.5451 - val_auc: 0.8023\n",
            "Epoch 3/50\n",
            "5059/5059 [==============================] - 220s 43ms/step - loss: 0.5467 - auc: 0.8005 - val_loss: 0.5288 - val_auc: 0.8233\n",
            "Epoch 4/50\n",
            "5059/5059 [==============================] - 197s 39ms/step - loss: 0.5323 - auc: 0.8127 - val_loss: 0.5038 - val_auc: 0.8347\n",
            "Epoch 5/50\n",
            "5059/5059 [==============================] - 185s 36ms/step - loss: 0.5263 - auc: 0.8182 - val_loss: 0.4984 - val_auc: 0.8385\n",
            "Epoch 6/50\n",
            "5059/5059 [==============================] - 198s 39ms/step - loss: 0.5187 - auc: 0.8238 - val_loss: 0.4997 - val_auc: 0.8390\n",
            "Epoch 7/50\n",
            "5059/5059 [==============================] - 231s 46ms/step - loss: 0.5206 - auc: 0.8233 - val_loss: 0.5333 - val_auc: 0.8178\n",
            "Epoch 8/50\n",
            "5059/5059 [==============================] - 233s 46ms/step - loss: 0.5195 - auc: 0.8236 - val_loss: 0.4981 - val_auc: 0.8406\n",
            "Epoch 9/50\n",
            "5059/5059 [==============================] - 2405s 475ms/step - loss: 0.5232 - auc: 0.8212 - val_loss: 0.5355 - val_auc: 0.8148\n",
            "Epoch 11/50\n",
            "5059/5059 [==============================] - 221s 44ms/step - loss: 0.5309 - auc: 0.8142 - val_loss: 0.5197 - val_auc: 0.8217\n",
            "Epoch 12/50\n",
            "5059/5059 [==============================] - 221s 44ms/step - loss: 0.5314 - auc: 0.8145 - val_loss: 0.5044 - val_auc: 0.8333\n",
            "Epoch 13/50\n",
            "5059/5059 [==============================] - 204s 40ms/step - loss: 0.5307 - auc: 0.8147 - val_loss: 0.5101 - val_auc: 0.8326\n",
            "Epoch 14/50\n",
            "5059/5059 [==============================] - 189s 37ms/step - loss: 0.5373 - auc: 0.8095 - val_loss: 0.5293 - val_auc: 0.8139\n",
            "Epoch 15/50\n",
            "5059/5059 [==============================] - 205s 41ms/step - loss: 0.5429 - auc: 0.8039 - val_loss: 0.5296 - val_auc: 0.8171\n",
            "Epoch 16/50\n",
            "5059/5059 [==============================] - 235s 47ms/step - loss: 0.5479 - auc: 0.7993 - val_loss: 0.5515 - val_auc: 0.7975\n",
            "Epoch 17/50\n",
            "5059/5059 [==============================] - 201s 40ms/step - loss: 0.5614 - auc: 0.7863 - val_loss: 0.5730 - val_auc: 0.7719\n",
            "Epoch 18/50\n",
            "5059/5059 [==============================] - 194s 38ms/step - loss: 0.5703 - auc: 0.7748 - val_loss: 0.5638 - val_auc: 0.7788\n",
            "Epoch 19/50\n",
            "5059/5059 [==============================] - 493s 97ms/step - loss: 0.5727 - auc: 0.7743 - val_loss: 0.5784 - val_auc: 0.7682\n",
            "Epoch 20/50\n",
            "5059/5059 [==============================] - 243s 48ms/step - loss: 0.5651 - auc: 0.7810 - val_loss: 0.5446 - val_auc: 0.8065\n",
            "Epoch 21/50\n",
            "5059/5059 [==============================] - 233s 46ms/step - loss: 0.5619 - auc: 0.7861 - val_loss: 0.5537 - val_auc: 0.7956\n",
            "Epoch 22/50\n",
            "5059/5059 [==============================] - 228s 45ms/step - loss: 0.5578 - auc: 0.7919 - val_loss: 0.5503 - val_auc: 0.8090\n",
            "Epoch 23/50\n",
            "5059/5059 [==============================] - 191s 38ms/step - loss: 0.5801 - auc: 0.7692 - val_loss: 0.5787 - val_auc: 0.7691\n",
            "Epoch 24/50\n",
            "5059/5059 [==============================] - 183s 36ms/step - loss: 0.5867 - auc: 0.7595 - val_loss: 0.5615 - val_auc: 0.7941\n",
            "Epoch 25/50\n",
            "5059/5059 [==============================] - 183s 36ms/step - loss: 0.5756 - auc: 0.7743 - val_loss: 0.5914 - val_auc: 0.7762\n",
            "Epoch 26/50\n",
            "5059/5059 [==============================] - 185s 37ms/step - loss: 0.5824 - auc: 0.7674 - val_loss: 0.5906 - val_auc: 0.7581\n",
            "Epoch 27/50\n",
            "5059/5059 [==============================] - 187s 37ms/step - loss: 0.5760 - auc: 0.7729 - val_loss: 0.5653 - val_auc: 0.7873\n",
            "Epoch 28/50\n",
            "5059/5059 [==============================] - 36731s 7s/step - loss: 0.5680 - auc: 0.7825 - val_loss: 0.5618 - val_auc: 0.7886\n",
            "Epoch 29/50\n",
            "5059/5059 [==============================] - 268s 53ms/step - loss: 0.5731 - auc: 0.7770 - val_loss: 0.5813 - val_auc: 0.7870\n",
            "Epoch 30/50\n",
            "5059/5059 [==============================] - 241s 48ms/step - loss: 0.5661 - auc: 0.7869 - val_loss: 0.5652 - val_auc: 0.7807\n",
            "Epoch 31/50\n",
            "5059/5059 [==============================] - 229s 45ms/step - loss: 0.5652 - auc: 0.7857 - val_loss: 0.5581 - val_auc: 0.7858\n",
            "Epoch 32/50\n",
            "5059/5059 [==============================] - 231s 46ms/step - loss: 0.5597 - auc: 0.7909 - val_loss: 0.5511 - val_auc: 0.8023\n",
            "Epoch 33/50\n",
            "5059/5059 [==============================] - 217s 43ms/step - loss: 0.5548 - auc: 0.7947 - val_loss: 0.5379 - val_auc: 0.8107\n",
            "Epoch 34/50\n",
            "5059/5059 [==============================] - 214s 42ms/step - loss: 0.5567 - auc: 0.7955 - val_loss: 0.5503 - val_auc: 0.8007\n",
            "Epoch 35/50\n",
            "5059/5059 [==============================] - 215s 42ms/step - loss: 0.5600 - auc: 0.7915 - val_loss: 0.5730 - val_auc: 0.7849\n",
            "Epoch 36/50\n",
            "5059/5059 [==============================] - 217s 43ms/step - loss: 0.5585 - auc: 0.7943 - val_loss: 0.5534 - val_auc: 0.8089\n",
            "Epoch 37/50\n",
            "5059/5059 [==============================] - 206s 41ms/step - loss: 0.5561 - auc: 0.7968 - val_loss: 0.5511 - val_auc: 0.8017\n",
            "Epoch 38/50\n",
            "5059/5059 [==============================] - 200s 40ms/step - loss: 0.5542 - auc: 0.7993 - val_loss: 0.5516 - val_auc: 0.8015\n",
            "Epoch 39/50\n",
            "5059/5059 [==============================] - 189s 37ms/step - loss: 0.5495 - auc: 0.8023 - val_loss: 0.5424 - val_auc: 0.8126\n",
            "Epoch 40/50\n",
            "5059/5059 [==============================] - 206s 41ms/step - loss: 0.5467 - auc: 0.8068 - val_loss: 0.5396 - val_auc: 0.8143\n",
            "Epoch 41/50\n",
            "5059/5059 [==============================] - 230s 45ms/step - loss: 0.5442 - auc: 0.8096 - val_loss: 0.5337 - val_auc: 0.8176\n",
            "Epoch 42/50\n",
            "5059/5059 [==============================] - 228s 45ms/step - loss: 0.5451 - auc: 0.8090 - val_loss: 0.5689 - val_auc: 0.8131\n",
            "Epoch 43/50\n",
            "5059/5059 [==============================] - 193s 38ms/step - loss: 0.5436 - auc: 0.8103 - val_loss: 0.5369 - val_auc: 0.8199\n",
            "Epoch 44/50\n",
            "5059/5059 [==============================] - 184s 36ms/step - loss: 0.5442 - auc: 0.8092 - val_loss: 0.5481 - val_auc: 0.8145\n",
            "Epoch 45/50\n",
            "5059/5059 [==============================] - 182s 36ms/step - loss: 0.5456 - auc: 0.8096 - val_loss: 0.5342 - val_auc: 0.8201\n",
            "Epoch 46/50\n",
            "5059/5059 [==============================] - 181s 36ms/step - loss: 0.5446 - auc: 0.8092 - val_loss: 0.5361 - val_auc: 0.8226\n",
            "Epoch 47/50\n",
            "5059/5059 [==============================] - 182s 36ms/step - loss: 0.5458 - auc: 0.8080 - val_loss: 0.5362 - val_auc: 0.8156\n",
            "Epoch 48/50\n",
            "5059/5059 [==============================] - 182s 36ms/step - loss: 0.5433 - auc: 0.8109 - val_loss: 0.5341 - val_auc: 0.8261\n",
            "Epoch 49/50\n",
            "5059/5059 [==============================] - 5696s 1s/step - loss: 0.5413 - auc: 0.8125 - val_loss: 0.5362 - val_auc: 0.8204\n",
            "Epoch 50/50\n",
            "5059/5059 [==============================] - 232s 46ms/step - loss: 0.5396 - auc: 0.8138 - val_loss: 0.5387 - val_auc: 0.8182\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x16620a147c0>"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "batch_size = 8\n",
        "num_batchs = math.ceil(len(training_balanced) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_balanced) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_balanced, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=50,\n",
        "    validation_data=gen_batch(\n",
        "        validation_balanced, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oIup_H3JsCAM"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_1.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "426Hs9fYsCAM"
      },
      "outputs": [],
      "source": [
        "from  tf2_gnn.layers.message_passing import rgat,rgin,rgcn,gnn_film,ggnn,gnn_edge_mlp\n",
        "#importing tensorflow and other libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.math import segment_mean #to calculate segmented mean\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, Model #layers and model\n",
        "from tensorflow.keras.layers import Embedding, Dense #layers\n",
        "from tensorflow.keras.optimizers import Adam #optimizer\n",
        "\n",
        "\n",
        "\n",
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 75)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"hidden_dim\"] = 32\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "fc1 = Dense(8,activation='relu')(avg)\n",
        "pred = Dense(1, activation='sigmoid')(fc1)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tgxrvEO6sCAN"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")\n",
        "import math\n",
        "\n",
        "batch_size = 64\n",
        "\n",
        "num_batchs = math.ceil(len(training_balanced) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_balanced) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_balanced, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=2,\n",
        "    validation_data=gen_batch(\n",
        "        validation_balanced, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z04j_70GsCAO"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_2.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uGysO-RsCAP"
      },
      "outputs": [],
      "source": [
        "from  tf2_gnn.layers.message_passing import RGAT,  MessagePassing, MessagePassingInput\n",
        "#importing tensorflow and other libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.math import segment_mean #to calculate segmented mean\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import Input, Model #layers and model\n",
        "from tensorflow.keras.layers import Embedding, Dense #layers\n",
        "from tensorflow.keras.optimizers import Adam #optimizer\n",
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 100)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"message_calculation_class\"] = 'rgat'\n",
        "params[\"num_heads\"] = 3\n",
        "params[\"hidden_dim\"] = 12\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "pred = Dense(1, activation='sigmoid')(avg)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X3lPdC-EsCAQ"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")\n",
        "import math\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "num_batchs = math.ceil(len(training_balanced) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_balanced) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_balanced, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=50,\n",
        "    validation_data=gen_batch(\n",
        "        validation_balanced, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UV6QebXsCAQ"
      },
      "outputs": [],
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_3.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g-qre2DRsCAR"
      },
      "outputs": [],
      "source": [
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 100)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"hidden_dim\"] = 40\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "\n",
        "pred = Dense(1, activation='sigmoid')(avg)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB-2W305sCAR"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uqWkZt7KsCAS"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "#splitting the train data\n",
        "training_set, validation_set = train_test_split(training_set, test_size=0.15,)\n",
        "batch_size = 8\n",
        "\n",
        "num_batchs = math.ceil(len(training_set) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_set, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=50,\n",
        "    validation_data=gen_batch(\n",
        "        validation_set, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_4.csv')"
      ],
      "metadata": {
        "id": "YwpElzfL0itM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 75)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"hidden_dim\"] = 32\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "fc1 = Dense(8,activation='relu')(avg)\n",
        "pred = Dense(1, activation='sigmoid')(fc1)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NAbjKOtp3e6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")"
      ],
      "metadata": {
        "id": "ubE9rJrr4UZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "#splitting the train data\n",
        "training_set, validation_set = train_test_split(training_set, test_size=0.15,)\n",
        "batch_size = 64\n",
        "\n",
        "num_batchs = math.ceil(len(training_set) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_set, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=2,\n",
        "    validation_data=gen_batch(\n",
        "        validation_set, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ],
      "metadata": {
        "id": "IMjy6Ln-4fxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_5.csv')"
      ],
      "metadata": {
        "id": "FMkzjXlp4jA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 100)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"message_calculation_class\"] = 'rgat'\n",
        "params[\"num_heads\"] = 3\n",
        "params[\"hidden_dim\"] = 12\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "pred = Dense(1, activation='sigmoid')(avg)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "NWtDaSbk5pGM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")"
      ],
      "metadata": {
        "id": "z8xe73Sv6Ia6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "#splitting the train data\n",
        "training_set, validation_set = train_test_split(training_set, test_size=0.15,)\n",
        "batch_size = 16\n",
        "\n",
        "num_batchs = math.ceil(len(training_set) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_set, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=2,\n",
        "    validation_data=gen_batch(\n",
        "        validation_set, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ],
      "metadata": {
        "id": "Vx7u1PNK6NV_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_6.csv')"
      ],
      "metadata": {
        "id": "pdVUJ0HO4m_g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 100)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"message_calculation_class\"] = 'rgat'\n",
        "params[\"num_heads\"] = 3\n",
        "params[\"hidden_dim\"] = 12\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "pred = Dense(1, activation='sigmoid')(avg)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "AlF8o9z69rXV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")"
      ],
      "metadata": {
        "id": "bGu7uH4T9KON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "#splitting the train data\n",
        "training_set, validation_set = train_test_split(training_set, test_size=0.15,)\n",
        "batch_size = 16\n",
        "\n",
        "num_batchs = math.ceil(len(training_set) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_set, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=30,\n",
        "    validation_data=gen_batch(\n",
        "        validation_set, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ],
      "metadata": {
        "id": "Gz0GOnhS8iLx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_7.csv')"
      ],
      "metadata": {
        "id": "au7frCXP8Wr0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 75)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"hidden_dim\"] = 32\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "fc1 = Dense(8,activation='relu')(avg)\n",
        "pred = Dense(1, activation='sigmoid')(fc1)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "SA6HK9vk91FB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")"
      ],
      "metadata": {
        "id": "_txGTard9M8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "#splitting the train data\n",
        "training_set, validation_set = train_test_split(training_set, test_size=0.15,)\n",
        "batch_size = 16\n",
        "\n",
        "num_batchs = math.ceil(len(training_set) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_set) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_set, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=10,\n",
        "    validation_data=gen_batch(\n",
        "        validation_set, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ],
      "metadata": {
        "id": "mJzAsWZl8b6T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_8.csv')"
      ],
      "metadata": {
        "id": "9sq7byFZ8WOv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 100)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"hidden_dim\"] = 40\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "\n",
        "pred = Dense(1, activation='sigmoid')(avg)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kG-plMXS-IME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")"
      ],
      "metadata": {
        "id": "x9ovBFyy9Omd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 64\n",
        "num_batchs = math.ceil(len(training_balanced) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_balanced) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_balanced, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=10,\n",
        "    validation_data=gen_batch(\n",
        "        validation_balanced, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ],
      "metadata": {
        "id": "55crecBu8eXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_9.csv')"
      ],
      "metadata": {
        "id": "IC25t98O8Vpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = keras.Input(batch_shape=(None,))\n",
        "edge = keras.Input(batch_shape=(None, 2), dtype=tf.int32)\n",
        "node2graph = keras.Input(batch_shape=(None,), dtype=tf.int32)\n",
        "embeded = Embedding(tokenizer.num_words, 100)(data)\n",
        "num_graph = tf.reduce_max(node2graph)+1\n",
        "gnn_input = GNNInput(\n",
        "    node_features=embeded,\n",
        "    adjacency_lists=(edge,),\n",
        "    node_to_graph_map=node2graph,\n",
        "    num_graphs=num_graph,\n",
        ")\n",
        "params = GNN.get_default_hyperparameters()\n",
        "params[\"message_calculation_class\"] = 'rgat'\n",
        "params[\"num_heads\"] = 3\n",
        "params[\"hidden_dim\"] = 12\n",
        "gnn_layer = GNN(params)\n",
        "#gnn output layer\n",
        "gnn_out = gnn_layer(gnn_input)\n",
        "print('gnn_out', gnn_out)\n",
        "avg = segment_mean(\n",
        "    data=gnn_out,\n",
        "    segment_ids=node2graph\n",
        "    )\n",
        "print('mean:', avg)\n",
        "pred = Dense(1, activation='sigmoid')(avg)\n",
        "print('pred:', pred)\n",
        "model = Model(\n",
        "    inputs={\n",
        "        'data': data,\n",
        "        'edges': edge,\n",
        "        'node2grah': node2graph,\n",
        "    },\n",
        "    outputs=pred\n",
        ")\n",
        "#printing summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "yz4KkebJ9oQg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    loss='BinaryCrossentropy',\n",
        "    metrics=['AUC']\n",
        ")"
      ],
      "metadata": {
        "id": "gTs3OD3E9Qcn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "batch_size = 64\n",
        "num_batchs = math.ceil(len(training_balanced) / batch_size)\n",
        "\n",
        "num_batchs_validation = math.ceil(len(validation_balanced) / batch_size)\n",
        "\n",
        "model.fit(\n",
        "    gen_batch(\n",
        "        training_balanced, batch_size=batch_size, repeat=True\n",
        "    ),\n",
        "    steps_per_epoch=num_batchs,\n",
        "    epochs=10,\n",
        "    validation_data=gen_batch(\n",
        "        validation_balanced, batch_size=16, repeat=True\n",
        "    ),\n",
        "    validation_steps=num_batchs_validation,\n",
        ")"
      ],
      "metadata": {
        "id": "hrkpis148fws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(\n",
        "    gen_batch(testing_set, batch_size=16, shuffle=False)\n",
        ")\n",
        "y_pred = np.reshape(y_pred, -1)\n",
        "len(y_pred)\n",
        "import pandas as pd\n",
        "submission = pd.DataFrame({'label':y_pred})\n",
        "submission.index.name = 'id'\n",
        "submission.to_csv('trail_10.csv')"
      ],
      "metadata": {
        "id": "odBiAi7K8U7u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Based on the provided template, describe the format of the input file (sdf file).\n",
        "The input file is structure data file (SDF). It contains information about the chemical composition of a molecule. SDF file store information about position of individual atom in the chemical compound and also tells about the connections. Bond block block tells about the bonding structure of the compound. These both blocks are used in this assignment to get information about the compound and saving them in form of edges and nodes. Each node is the atom given in the chemical molecule.Different molecules are delimited by '$$$$' expression.\n",
        "Each sample/molecule starts with header which tells about the name/title of the compound. Other sections includes information about Atom count, version number, connections etc. Atom block tells about the elements of the compound.\n",
        "\n",
        "# What are the input tensors to the neural network model (their meaning, not just symbol)? What is each of their dims and their meaning (e.g. batch_size?)?\n",
        " The input tensors :\n",
        "data: The shape for each batch is batch_size*max_len_nodes, where batch_size is the number of samples in the batch and max_len_nodes is the length of tokenized nodes after padding is done. The data contains the nodes of the chemical compound in the tokenized form. Nodes for each compound are extracted, then they are tokenized using the tokenizer and finally padding is done using pad_sequence method.\n",
        "node2graph:  The shape for each batch is batch_size*max_len_nodes, where batch_size is the number of samples in the batch and max_len_nodes is the length of tokenized nodes after padding is done.\n",
        "It is the input tensor which is used for segmented mean and contains information about segmented ids.\n",
        "edge:The shape of edge is sum_of_all_edges,2. The sum_of_all_edges represents the sum(no. of edges of each sample) of the batch_size. For example in a batch of 3 samples, the number of edges in sample 1: 21, sample 2: 20 and sample 3: 40. So the size of edge tensor would be 81,2. edge is the input tensor which carries information about connections between atoms.\n",
        "\n",
        "# For each dim of gnn_out, what does it represent? For each dim of avg, what does it represent?\n",
        "gnn_out: The gnn_out is of shape batch_size_node_dimension,hidden layers, It represents the aggregation output of the model for each hidden layer. where batch_size_node_dimension is the dimension of the input data (node) vector (dimension of tokenized vector for the complete batch).\n",
        "\n",
        "avg:  The final output of the avg tensor is of shape batch_size, hidden_layer. It is a way of collecting information for each sample and representing it in the form of mean data. Each sample has one segment id. Thus the segment_mean takes the mean of all the output data in the gnn_out output and represents one sample with one number for each hidden layer.Average takes the segmented mean of the gnn_out based on the segmented ids. For each sample in the batch_size, the output of gnn_out is tokenized_vector_dimension, hidden_layers.\n",
        "\n",
        "#What is the difference between segment_mean and tf.reduce_mean? For each dim of pred, what does it represent?\n",
        " segment_mean takes the mean of the data which have same segmented ids.\n",
        "pred:  the final output is a number which represents the probability associated with each chemical compound about its activity.The final output (pred) tells about the probability of a chemical compound to be active for the cancer cell or not. The shape of pred is batch_size,1. Thus for each sample.\n",
        "reduce_mean: computes the mean of elements across dimensions of a tensor given the arguments.\n",
        "\n",
        "\n",
        "\n",
        "#What is the motivation / theory/idea to use multiple gcn layers comparing to just one? How many layers were used in the template?\n",
        "The default layer are 4 as given in the documentaion. The default message passing method is rgcn (Graph convolution layers). Using multiple gcn helps in incorporating all the graph complexity properly and thus creates a better model.The default template implements the default setting of the number of layers in the gcn network."
      ],
      "metadata": {
        "id": "xBw_2wx9vCCG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Problem Description: It is a binary classification problem based on the graph data. The task is to predict the anticancer activity of a chemical compound using the chemical structure of the compound. The chemical compound can be positive or negative against lung cancer cell and thus labelled as either 0 or 1.The data is in the form of graph which represents the chemical structure of the compound. Each sample of data contains information about the atoms and the connections between atoms of the molecule. So in this problem the features are the atoms and connections.\n",
        "\n",
        "Methods\n",
        "The nodes(atoms) are given as characters . Thus it is treated as sequence of text data and best way to describe the text data sequence to tokenize the data and then adding the embeddig layer.\n",
        "The first step is to read the sdf file to get the information about the atoms and their connectivity in the compound. The atoms are described as nodes and connections are described as edges. The read_sdf method is used to read sdf file and the chemical composition of the compound.\n",
        "Graph convolutional network is used in this assignment to calculate the probability of the output class. Different methods differ in implementing message passing methods as:\n",
        "R-GIN Method: (Relation Graph Isomorphism Network message propogation layer) Compute new graph states by neural message passing using MLPs for state updates and message computation.\n",
        "GGNN Method: (Gated graph neural network layer) Compute new graph states by neural message passing and gated units on the nodes. This method works best for this problem as given in Aggregation Method 2 below.\n",
        "R-GCN Method: (Graph convolution layers) Compute new graph states by neural message passing.\n",
        "R-GAT Method: (Relation graph attention network layer) Compute new graph states by neural message passing using attention.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hQPezRlbxIgp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gtPXWYAbw4Z4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}